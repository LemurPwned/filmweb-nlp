{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LemurPwned/filmweb-nlp/blob/master/Transformers_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91gigFqrlFzu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0lfw5T-kjlz",
        "colab_type": "code",
        "outputId": "246a0d12-fab2-45c1-cd6d-b8476a41a4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.11.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY_hw-16kXkq",
        "colab_type": "code",
        "outputId": "39328b32-5005-4df8-ce09-6a2bbc34aa1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "import pandas as pd \n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "model_name = 'bert-base-multilingual-cased'\n",
        "merge = True\n",
        "if merge:\n",
        "  df = pd.DataFrame()\n",
        "  for filename in ['reviews_for_bert.csv', 'reviews_for_bert_150_150.csv',\n",
        "                   'reviews_for_bert_50_150.csv',\n",
        "                   'reviews_for_bert_300_150.csv']:\n",
        "    tmp = pd.read_csv(filename)\n",
        "    df = pd.concat((df, tmp))\n",
        "else:\n",
        "  df = pd.read_csv('/content/reviews_for_bert.csv')\n",
        "labels = df['rating'].unique().tolist()\n",
        "if 0 not in labels:\n",
        "  # labels must start from 0\n",
        "  labels = [label - 1 for label in labels]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                      num_labels=len(labels))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995526/995526 [00:00<00:00, 5805180.59B/s]\n",
            "100%|██████████| 521/521 [00:00<00:00, 263281.01B/s]\n",
            "100%|██████████| 714314041/714314041 [00:13<00:00, 51588527.91B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D64xl7q4gotq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers.data.processors.utils import InputExample\n",
        "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np \n",
        "import torch\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_LEN = 128\n",
        "\n",
        "\n",
        "def select_field(features, field):\n",
        "  return [\n",
        "        [\n",
        "            feature[field]\n",
        "        ]\n",
        "        for feature in features\n",
        "    ]\n",
        "  \n",
        "def prepare_dataset(df):\n",
        "  df_sampl = df\n",
        "  sents = df_sampl['content'].values\n",
        "  labels = df_sampl['rating'].astype(int).values\n",
        "  labels_unique = df['rating'].astype(int).unique()\n",
        "  # print(labels_unique)\n",
        "  # print(labels)\n",
        "  # label_map = {i: i for i in range(11)}\n",
        "  guid =0 \n",
        "  input_examples = []\n",
        "  for sent, label in zip(sents, labels):\n",
        "    input_examples.append(InputExample(\n",
        "        guid, text_a=sent, text_b=None, label=label\n",
        "    ))\n",
        "  tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "  features = glue_convert_examples_to_features(\n",
        "      input_examples,\n",
        "      tokenizer=tokenizer,\n",
        "      max_length=MAX_SEQ_LEN,\n",
        "      output_mode='classification',\n",
        "      label_list = labels_unique\n",
        "  )\n",
        "\n",
        "\n",
        "  all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "  all_input_mask = torch.tensor([feature.attention_mask for feature in features], dtype=torch.long)\n",
        "  all_segment_ids = torch.tensor([feature.token_type_ids for feature in features], dtype=torch.long)\n",
        "  all_label_ids = torch.tensor([feature.label for feature in features], dtype=torch.long)\n",
        "\n",
        "  dataset = TensorDataset(all_input_ids, \n",
        "                          all_input_mask, \n",
        "                          all_segment_ids, \n",
        "                          all_label_ids)\n",
        "  return dataset \n",
        "\n",
        "msk = np.random.rand(len(df)) < 0.8\n",
        "train = df[msk]\n",
        "test = df[~msk]\n",
        "train_datatset = prepare_dataset(train)\n",
        "test_dataset = prepare_dataset(test)\n",
        "train_dataloader = data_utils.DataLoader(train_datatset, \n",
        "                                         batch_size=BATCH_SIZE, \n",
        "                                         shuffle=True)\n",
        "test_dataloader = data_utils.DataLoader(test_dataset, \n",
        "                                        batch_size=BATCH_SIZE, \n",
        "                                        shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn4SXhD7qziZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "lr = 2e-5\n",
        "warmup=0.1\n",
        "\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 1000\n",
        "num_warmup_steps = 100\n",
        "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False,\n",
        "                  # )  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler\n",
        "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=num_warmup_steps, \n",
        "                                            num_training_steps=num_total_steps)  # PyTorch scheduler\n",
        "### and used like this:\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5ncBAcBtxNw",
        "colab_type": "code",
        "outputId": "436efcf3-40a4-44a3-922b-0c17cdf5461a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from tqdm import trange \n",
        "### and used like this:\n",
        "# for batch in train_data:\n",
        "#     loss = model(batch)\n",
        "#     loss.backward()\n",
        "#     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "#     optimizer.step()\n",
        "#     scheduler.step()\n",
        "#     optimizer.zero_grad()\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "EPOCHS_NUM = 4\n",
        "loss_history = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                      num_labels=len(labels))\n",
        "model.cuda()\n",
        "\n",
        "for epoch in trange(EPOCHS_NUM, desc='EPOCH'):\n",
        "  # set to training mode\n",
        "  model.train()\n",
        "  total_loss =0\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_segment_ids, b_labels = batch\n",
        "    outputs = model(b_input_ids, \n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "    loss = outputs[0]\n",
        "    loss_history.append(loss.item())\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    total_loss += loss.item()\n",
        "  print(f\"Total loss is {total_loss}\")\n",
        "\n",
        "  model.eval()\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps = 0\n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_segment_ids, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask)[0]\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEPOCH:   0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total loss is 304.87768745422363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEPOCH:  25%|██▌       | 1/4 [03:07<09:22, 187.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.008787878787878787\n",
            "Total loss is 304.9172909259796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEPOCH:  50%|█████     | 2/4 [06:15<06:15, 187.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.008522727272727272\n",
            "Total loss is 305.08406233787537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEPOCH:  75%|███████▌  | 3/4 [09:23<03:07, 187.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.008522727272727272\n",
            "Total loss is 304.8327236175537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEPOCH: 100%|██████████| 4/4 [12:32<00:00, 187.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.008522727272727272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KHiDH1qY1B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}